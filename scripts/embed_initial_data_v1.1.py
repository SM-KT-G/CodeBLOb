"""
v1.1 ì„ë² ë”© íŒŒì´í”„ë¼ì¸ - Parent-Child ì•„í‚¤í…ì²˜
- e5-small (384ì°¨ì›) + ë°°ì¹˜ ìµœì í™”
- Parent/Child ë¶„ë¦¬ ì €ì¥
- ì²´í¬í¬ì¸íŠ¸ ë³µêµ¬
"""
import os
import sys
import json
import asyncio
import psycopg
import torch
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# embedding_utils_v1.1.pyì—ì„œ í•¨ìˆ˜ ì„í¬íŠ¸
import importlib.util
spec = importlib.util.spec_from_file_location(
    "embedding_utils_v11",
    project_root / "scripts" / "embedding_utils_v1.1.py"
)
embedding_utils = importlib.util.module_from_spec(spec)
spec.loader.exec_module(embedding_utils)

process_json_file = embedding_utils.process_json_file
create_embedding_text_for_child = embedding_utils.create_embedding_text_for_child
calculate_statistics = embedding_utils.calculate_statistics


# ========================================
# ì„¤ì •
# ========================================
DATA_DIR = project_root / 'labled_data'
CHECKPOINT_FILE = project_root / 'scripts' / 'embedding_checkpoint_v1.1.json'

# PostgreSQL ì—°ê²° ì •ë³´
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'tourism_db',
    'user': 'tourism_user',
    'password': 'tourism_pass'
}

# ì„ë² ë”© ì„¤ì •
MODEL_NAME = 'intfloat/multilingual-e5-small'  # 384 dims
BATCH_SIZE = 32  # ì‘ê²Œ ì‹œì‘ (í…ŒìŠ¤íŠ¸)
DEVICE = 'mps' if torch.backends.mps.is_available() else 'cpu'

print(f"ğŸ”§ ì„ë² ë”© ì„¤ì •:")
print(f"   ëª¨ë¸: {MODEL_NAME} (384 dims)")
print(f"   ë°°ì¹˜: {BATCH_SIZE}")
print(f"   ë””ë°”ì´ìŠ¤: {DEVICE}")


# ========================================
# ëª¨ë¸ ë¡œë“œ
# ========================================
print(f"\nğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = SentenceTransformer(MODEL_NAME, device=DEVICE)
model.eval()
print(f"âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")


# ========================================
# DB ì—°ê²°
# ========================================
def get_db_connection():
    """PostgreSQL ì—°ê²°"""
    return psycopg.connect(**DB_CONFIG)


def init_database():
    """
    DB ì´ˆê¸°í™” (v1.1 ìŠ¤í‚¤ë§ˆ ì‹¤í–‰)
    """
    print(f"â„¹ï¸  ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ê±´ë„ˆëœ€ (ì´ë¯¸ ì‹¤í–‰ë¨)")
    return True


# ========================================
# Parent ì €ì¥
# ========================================
def save_parent_batch(parents: List[Dict]) -> Dict[str, int]:
    """
    Parent ë ˆì½”ë“œ ë°°ì¹˜ ì €ì¥
    
    Returns:
        {document_id: parent_db_id} ë§¤í•‘
    """
    if not parents:
        return {}
    
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            # INSERT ì¿¼ë¦¬
            insert_sql = """
                INSERT INTO tourism_parent (
                    document_id, domain, title, summary_text,
                    place_name, area, lang, source_type, source_url,
                    collected_date, published_date
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                )
                ON CONFLICT (document_id) DO NOTHING
                RETURNING id, document_id;
            """
            
            id_map = {}
            
            for parent in parents:
                cur.execute(insert_sql, (
                    parent['document_id'],
                    parent['domain'],
                    parent['title'],
                    parent['summary_text'],
                    parent['place_name'],
                    parent['area'],
                    parent['lang'],
                    parent['source_type'],
                    parent['source_url'],
                    parent['collected_date'],
                    parent['published_date']
                ))
                
                result = cur.fetchone()
                if result:
                    parent_id, doc_id = result
                    id_map[doc_id] = parent_id
            
            conn.commit()
    
    return id_map


# ========================================
# Child ì €ì¥ (ì„ë² ë”© í¬í•¨)
# ========================================
def save_child_batch(children: List[Dict], embeddings: List[List[float]]):
    """
    Child ì²­í¬ ë°°ì¹˜ ì €ì¥ (ì„ë² ë”© í¬í•¨)
    """
    if not children or not embeddings:
        return
    
    if len(children) != len(embeddings):
        raise ValueError(f"ì²­í¬({len(children)})ì™€ ì„ë² ë”©({len(embeddings)}) ìˆ˜ ë¶ˆì¼ì¹˜")
    
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            insert_sql = """
                INSERT INTO tourism_child (
                    qa_id, parent_id, document_id,
                    question, answer, chunk_text,
                    domain, title, place_name, area, lang,
                    embedding
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                )
                ON CONFLICT (qa_id) DO NOTHING;
            """
            
            for child, emb in zip(children, embeddings):
                cur.execute(insert_sql, (
                    child['qa_id'],
                    child['parent_id'],
                    child['document_id'],
                    child['question'],
                    child['answer'],
                    child['chunk_text'],
                    child['domain'],
                    child['title'],
                    child['place_name'],
                    child['area'],
                    child['lang'],
                    emb
                ))
            
            conn.commit()


# ========================================
# íŒŒì¼ ìˆ˜ì§‘
# ========================================
def collect_json_files() -> List[Path]:
    """ëª¨ë“  JSON íŒŒì¼ ìˆ˜ì§‘"""
    json_files = []
    
    for domain_dir in DATA_DIR.iterdir():
        if domain_dir.is_dir() and domain_dir.name.startswith('TL_'):
            json_files.extend(domain_dir.glob('*.json'))
    
    return sorted(json_files)


# ========================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ========================================
def load_checkpoint() -> Dict:
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    if CHECKPOINT_FILE.exists():
        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        'processed_files': [],
        'total_parents': 0,
        'total_children': 0,
        'last_updated': None
    }


def save_checkpoint(checkpoint: Dict):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    checkpoint['last_updated'] = datetime.now().isoformat()
    with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
        json.dump(checkpoint, f, ensure_ascii=False, indent=2)


# ========================================
# ë©”ì¸ ì„ë² ë”© íŒŒì´í”„ë¼ì¸
# ========================================
async def embed_all_files():
    """
    ì „ì²´ íŒŒì¼ ì„ë² ë”© íŒŒì´í”„ë¼ì¸
    
    1. Parent ë°°ì¹˜ ì €ì¥
    2. Child í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    3. ë°°ì¹˜ ì„ë² ë”©
    4. Child + ì„ë² ë”© ì €ì¥
    """
    # 1. íŒŒì¼ ìˆ˜ì§‘
    print(f"\nğŸ“‚ JSON íŒŒì¼ ìˆ˜ì§‘ ì¤‘...")
    all_files = collect_json_files()
    print(f"âœ… ì´ {len(all_files):,}ê°œ íŒŒì¼ ë°œê²¬")
    
    # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = load_checkpoint()
    processed_set = set(checkpoint['processed_files'])
    remaining_files = [f for f in all_files if str(f) not in processed_set]
    
    print(f"ğŸ“Š ì§„í–‰ ìƒí™©:")
    print(f"   ì™„ë£Œ: {len(processed_set):,}ê°œ")
    print(f"   ë‚¨ìŒ: {len(remaining_files):,}ê°œ")
    
    if not remaining_files:
        print(f"\nâœ… ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
        return
    
    # 3. ë°°ì¹˜ ì²˜ë¦¬
    parent_batch = []
    child_batch = []
    all_child_chunks = []  # ëª¨ë“  child ì €ì¥
    processed_count = len(processed_set)
    
    for filepath in tqdm(remaining_files, desc="ì„ë² ë”© ì§„í–‰"):
        # JSON íŒŒì‹±
        parent_data, child_chunks = process_json_file(filepath)
        
        if not parent_data:
            continue
        
        parent_batch.append(parent_data)
        all_child_chunks.extend(child_chunks)  # child ëˆ„ì 
        
        # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ ì €ì¥
        if len(parent_batch) >= BATCH_SIZE:
            # Parent ì €ì¥ ë° ID ë§¤í•‘
            id_map = save_parent_batch(parent_batch)
            
            # Childì— parent_id ë§¤í•‘
            for child in all_child_chunks:
                doc_id = child['document_id']
                parent_db_id = id_map.get(doc_id)
                
                if parent_db_id:
                    child['parent_id'] = parent_db_id
                    child_batch.append(child)
            
            # Child ì„ë² ë”© + ì €ì¥
            if child_batch:
                # ì„ë² ë”© í…ìŠ¤íŠ¸ ì¶”ì¶œ
                texts = [create_embedding_text_for_child(c) for c in child_batch]
                
                # ë°°ì¹˜ ì„ë² ë”©
                with torch.no_grad():
                    embeddings = model.encode(
                        texts,
                        batch_size=min(BATCH_SIZE, len(texts)),
                        show_progress_bar=False,
                        convert_to_numpy=True
                    )
                
                # ì €ì¥
                save_child_batch(child_batch, embeddings.tolist())
                
                # ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
                checkpoint['total_parents'] += len(parent_batch)
                checkpoint['total_children'] += len(child_batch)
            
            # ë°°ì¹˜ ì´ˆê¸°í™”
            for parent in parent_batch:
                checkpoint['processed_files'].append(str(filepath))
            
            processed_count += len(parent_batch)
            parent_batch = []
            child_batch = []
            all_child_chunks = []
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (1000ê°œë§ˆë‹¤)
            if processed_count % 1000 == 0:
                save_checkpoint(checkpoint)
    
    # 4. ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
    if parent_batch:
        id_map = save_parent_batch(parent_batch)
        
        for child in all_child_chunks:
            doc_id = child['document_id']
            parent_db_id = id_map.get(doc_id)
            
            if parent_db_id:
                child['parent_id'] = parent_db_id
                child_batch.append(child)
        
        if child_batch:
            texts = [create_embedding_text_for_child(c) for c in child_batch]
            
            with torch.no_grad():
                embeddings = model.encode(
                    texts,
                    batch_size=min(BATCH_SIZE, len(texts)),
                    show_progress_bar=False,
                    convert_to_numpy=True
                )
            
            save_child_batch(child_batch, embeddings.tolist())
            
            checkpoint['total_parents'] += len(parent_batch)
            checkpoint['total_children'] += len(child_batch)
        
        for parent in parent_batch:
            checkpoint['processed_files'].append(str(filepath))
    
    # 5. ìµœì¢… ì €ì¥
    save_checkpoint(checkpoint)
    
    print(f"\nâœ… ì„ë² ë”© ì™„ë£Œ!")
    print(f"   Parents: {checkpoint['total_parents']:,}ê°œ")
    print(f"   Children: {checkpoint['total_children']:,}ê°œ")


# ========================================
# ì‹¤í–‰
# ========================================
if __name__ == '__main__':
    print(f"ğŸš€ v1.1 ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì‹œì‘\n")
    
    # DB ì´ˆê¸°í™”
    if not init_database():
        sys.exit(1)
    
    # ì„ë² ë”© ì‹¤í–‰
    asyncio.run(embed_all_files())
    
    print(f"\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
