"""
v2.0 ë¹„ë™ê¸° ì„ë² ë”© íŒŒì´í”„ë¼ì¸
- e5-small (384ì°¨ì›)
- asyncio + ThreadPoolExecutorë¡œ DB I/Oì™€ GPU ì—°ì‚° ë³‘ë ¬ ì²˜ë¦¬
- Parent/Child ë¶„ë¦¬ ì €ì¥
"""
import os
import sys
import json
import asyncio
import psycopg
import torch
import traceback
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€ (scripts/v2/ -> root)
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# embedding_utils_v1.1.pyì—ì„œ í•¨ìˆ˜ ì„í¬íŠ¸
try:
    import importlib.util
    utils_path = project_root / "scripts" / "embedding_utils_v1.1.py"
    spec = importlib.util.spec_from_file_location(
        "embedding_utils_v11",
        utils_path
    )
    embedding_utils = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(embedding_utils)

    process_json_file = embedding_utils.process_json_file
    create_embedding_text_for_child = embedding_utils.create_embedding_text_for_child
    calculate_statistics = embedding_utils.calculate_statistics
    print("âœ… embedding_utils ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âŒ embedding_utils ë¡œë“œ ì‹¤íŒ¨: {e}")
    traceback.print_exc()
    sys.exit(1)


# ========================================
# ì„¤ì •
# ========================================
DATA_DIR = project_root / 'labled_data'
# ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë„ v2 í´ë”ì— ì €ì¥
CHECKPOINT_FILE = Path(__file__).parent / 'embedding_checkpoint_v2.json'

# PostgreSQL ì—°ê²° ì •ë³´
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'tourism_db',
    'user': 'citsk',
    'password': 'citsk!',
    'connect_timeout': 5
}

# ì„ë² ë”© ì„¤ì •
MODEL_NAME = 'intfloat/multilingual-e5-small'  # 384 dims
BATCH_SIZE = 256  # GPU ì„±ëŠ¥ ê³ ë ¤í•˜ì—¬ ì¦ëŸ‰
DEVICE = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')

print(f"ğŸ”§ ì„ë² ë”© ì„¤ì •:")
print(f"   ëª¨ë¸: {MODEL_NAME} (384 dims)")
print(f"   ë°°ì¹˜: {BATCH_SIZE}")
print(f"   ë””ë°”ì´ìŠ¤: {DEVICE}")


# ========================================
# ëª¨ë¸ ë¡œë“œ
# ========================================
try:
    print(f"\nğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = SentenceTransformer(MODEL_NAME, device=DEVICE)
    model.eval()
    print(f"âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    traceback.print_exc()
    sys.exit(1)


# ========================================
# DB ì—°ê²°
# ========================================
def get_db_connection():
    """PostgreSQL ì—°ê²°"""
    return psycopg.connect(**DB_CONFIG)


def init_database():
    """
    DB ì´ˆê¸°í™” (v1.1 ìŠ¤í‚¤ë§ˆ ì‹¤í–‰)
    """
    print("ğŸ”„ DB ì—°ê²° ì‹œë„ ì¤‘...")
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT 1")
        print(f"âœ… DB ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
        print(f"   ì„¤ì •: {DB_CONFIG}")
        traceback.print_exc()
        return False

    print(f"â„¹ï¸  ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ê±´ë„ˆëœ€ (ì´ë¯¸ ì‹¤í–‰ë¨)")
    return True


# ========================================
# Parent ì €ì¥
# ========================================
def save_parent_batch(parents: List[Dict]) -> Dict[str, int]:
    """
    Parent ë ˆì½”ë“œ ë°°ì¹˜ ì €ì¥
    
    Returns:
        {document_id: parent_db_id} ë§¤í•‘
    """
    if not parents:
        return {}
    
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            # INSERT ì¿¼ë¦¬
            insert_sql = """
                INSERT INTO tourism_parent (
                    document_id, domain, title, summary_text,
                    place_name, area, lang, source_type, source_url,
                    collected_date, published_date
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                )
                ON CONFLICT (document_id) DO NOTHING
                RETURNING id, document_id;
            """
            
            id_map = {}
            
            for parent in parents:
                cur.execute(insert_sql, (
                    parent['document_id'],
                    parent['domain'],
                    parent['title'],
                    parent['summary_text'],
                    parent['place_name'],
                    parent['area'],
                    parent['lang'],
                    parent['source_type'],
                    parent['source_url'],
                    parent['collected_date'],
                    parent['published_date']
                ))
                
                result = cur.fetchone()
                if result:
                    parent_id, doc_id = result
                    id_map[doc_id] = parent_id
            
            conn.commit()
    
    return id_map


# ========================================
# Child ì €ì¥ (ì„ë² ë”© í¬í•¨)
# ========================================
def save_child_batch(children: List[Dict], embeddings: List[List[float]]):
    """
    Child ì²­í¬ ë°°ì¹˜ ì €ì¥ (ì„ë² ë”© í¬í•¨)
    """
    if not children or not embeddings:
        return
    
    if len(children) != len(embeddings):
        raise ValueError(f"ì²­í¬({len(children)})ì™€ ì„ë² ë”©({len(embeddings)}) ìˆ˜ ë¶ˆì¼ì¹˜")
    
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            insert_sql = """
                INSERT INTO tourism_child (
                    qa_id, parent_id, document_id,
                    question, answer, chunk_text,
                    domain, title, place_name, area, lang,
                    embedding
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                )
                ON CONFLICT (qa_id) DO NOTHING;
            """
            
            for child, emb in zip(children, embeddings):
                cur.execute(insert_sql, (
                    child['qa_id'],
                    child['parent_id'],
                    child['document_id'],
                    child['question'],
                    child['answer'],
                    child['chunk_text'],
                    child['domain'],
                    child['title'],
                    child['place_name'],
                    child['area'],
                    child['lang'],
                    emb
                ))
            
            conn.commit()


# ========================================
# íŒŒì¼ ìˆ˜ì§‘
# ========================================
def collect_json_files() -> List[Path]:
    """ëª¨ë“  JSON íŒŒì¼ ìˆ˜ì§‘"""
    json_files = []
    
    for domain_dir in DATA_DIR.iterdir():
        if domain_dir.is_dir() and domain_dir.name.startswith('TL_'):
            json_files.extend(domain_dir.glob('*.json'))
    
    return sorted(json_files)


# ========================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ========================================
def load_checkpoint() -> Dict:
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    if CHECKPOINT_FILE.exists():
        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        'processed_files': [],
        'total_parents': 0,
        'total_children': 0,
        'last_updated': None
    }


def save_checkpoint(checkpoint: Dict):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    checkpoint['last_updated'] = datetime.now().isoformat()
    with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
        json.dump(checkpoint, f, ensure_ascii=False, indent=2)


# ========================================
# ë©”ì¸ ì„ë² ë”© íŒŒì´í”„ë¼ì¸
# ========================================
async def embed_all_files():
    """
    ì „ì²´ íŒŒì¼ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ (ë¹„ë™ê¸° ìµœì í™”)
    """
    # 1. íŒŒì¼ ìˆ˜ì§‘
    print(f"\nğŸ“‚ JSON íŒŒì¼ ìˆ˜ì§‘ ì¤‘...")
    all_files = collect_json_files()
    print(f"âœ… ì´ {len(all_files):,}ê°œ íŒŒì¼ ë°œê²¬")
    
    # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = load_checkpoint()
    processed_set = set(checkpoint['processed_files'])
    remaining_files = [f for f in all_files if str(f) not in processed_set]
    
    print(f"ğŸ“Š ì§„í–‰ ìƒí™©:")
    print(f"   ì™„ë£Œ: {len(processed_set):,}ê°œ")
    print(f"   ë‚¨ìŒ: {len(remaining_files):,}ê°œ")
    
    if not remaining_files:
        print(f"\nâœ… ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
        return
    
    # 3. ë°°ì¹˜ ì²˜ë¦¬
    parent_batch = []
    child_batch = []
    all_child_chunks = []  # ëª¨ë“  child ì €ì¥
    processed_count = len(processed_set)
    
    for filepath in tqdm(remaining_files, desc="ì„ë² ë”© ì§„í–‰"):
        try:
            # JSON íŒŒì‹±
            parent_data, child_chunks = process_json_file(filepath)
            
            if not parent_data:
                continue
            
            parent_batch.append(parent_data)
            all_child_chunks.extend(child_chunks)  # child ëˆ„ì 
            
            # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ ì €ì¥
            if len(parent_batch) >= BATCH_SIZE:
                # 1. ì„ë² ë”© í…ìŠ¤íŠ¸ ë¯¸ë¦¬ ì¤€ë¹„ (ID ì—†ì´ ê°€ëŠ¥)
                texts = [create_embedding_text_for_child(c) for c in all_child_chunks]
                
                # 2. ë³‘ë ¬ ì‹¤í–‰: Parent ì €ì¥(DB) vs ì„ë² ë”© ê³„ì‚°(GPU)
                # Parent ì €ì¥ì€ DB I/O ëŒ€ê¸°, ì„ë² ë”©ì€ GPU ì—°ì‚°
                future_db = asyncio.to_thread(save_parent_batch, parent_batch)
                
                # ì„ë² ë”© ê³„ì‚° (GPU)
                def run_embedding():
                    with torch.no_grad():
                        return model.encode(
                            texts,
                            batch_size=BATCH_SIZE,
                            show_progress_bar=False,
                            convert_to_numpy=True
                        ).tolist()
                
                future_emb = asyncio.to_thread(run_embedding)
                
                # ë‘ ì‘ì—… ë™ì‹œì— ì‹¤í–‰ ë° ëŒ€ê¸°
                id_map, embeddings = await asyncio.gather(future_db, future_emb)
                
                # 3. ê²°ê³¼ í•©ì¹˜ê¸° (Parent ID ë§¤í•‘)
                child_batch = []
                valid_embeddings = []
                
                for child, emb in zip(all_child_chunks, embeddings):
                    doc_id = child['document_id']
                    parent_db_id = id_map.get(doc_id)
                    
                    if parent_db_id:
                        child['parent_id'] = parent_db_id
                        child_batch.append(child)
                        valid_embeddings.append(emb)
                
                # 4. Child ì €ì¥ (DB)
                if child_batch:
                    await asyncio.to_thread(save_child_batch, child_batch, valid_embeddings)
                    
                    # ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
                    checkpoint['total_parents'] += len(parent_batch)
                    checkpoint['total_children'] += len(child_batch)
                
                # ë°°ì¹˜ ì´ˆê¸°í™”
                for parent in parent_batch:
                    checkpoint['processed_files'].append(str(filepath))
                
                processed_count += len(parent_batch)
                parent_batch = []
                child_batch = []
                all_child_chunks = []
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (1000ê°œë§ˆë‹¤)
                if processed_count % 1000 == 0:
                    save_checkpoint(checkpoint)
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ (íŒŒì¼: {filepath}): {e}")
            traceback.print_exc()
            sys.exit(1)
    
    # 4. ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
    if parent_batch:
        # ë§ˆì§€ë§‰ ë°°ì¹˜ëŠ” ë³‘ë ¬ ì²˜ë¦¬ë³´ë‹¤ëŠ” ìˆœì°¨ ì²˜ë¦¬ê°€ ì•ˆì „ (ë‚¨ì€ ì–‘ì´ ì ì„ ìˆ˜ ìˆìŒ)
        id_map = save_parent_batch(parent_batch)
        
        for child in all_child_chunks:
            doc_id = child['document_id']
            parent_db_id = id_map.get(doc_id)
            
            if parent_db_id:
                child['parent_id'] = parent_db_id
                child_batch.append(child)
        
        if child_batch:
            texts = [create_embedding_text_for_child(c) for c in child_batch]
            
            with torch.no_grad():
                embeddings = model.encode(
                    texts,
                    batch_size=min(BATCH_SIZE, len(texts)),
                    show_progress_bar=False,
                    convert_to_numpy=True
                )
            
            save_child_batch(child_batch, embeddings.tolist())
            
            checkpoint['total_parents'] += len(parent_batch)
            checkpoint['total_children'] += len(child_batch)
        
        for parent in parent_batch:
            checkpoint['processed_files'].append(str(filepath))
    
    # 5. ìµœì¢… ì €ì¥
    save_checkpoint(checkpoint)
    
    print(f"\nâœ… ì„ë² ë”© ì™„ë£Œ!")
    print(f"   Parents: {checkpoint['total_parents']:,}ê°œ")
    print(f"   Children: {checkpoint['total_children']:,}ê°œ")


# ========================================
# ì‹¤í–‰
# ========================================
if __name__ == '__main__':
    print(f"ğŸš€ v2.0 ë¹„ë™ê¸° ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì‹œì‘\n")
    
    # DB ì´ˆê¸°í™”
    if not init_database():
        sys.exit(1)
    
    # ì„ë² ë”© ì‹¤í–‰
    asyncio.run(embed_all_files())
    
    print(f"\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
